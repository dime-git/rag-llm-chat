# RAG LLM Chat

A powerful chat application that combines Large Language Models (LLMs) with Retrieval Augmented Generation (RAG) capabilities. Built with Streamlit, supporting both OpenAI and Anthropic models.

## Features

- ðŸ¤– Multi-model support (OpenAI GPT-4 and Anthropic Claude)
- ðŸ“š RAG capabilities with document processing
- ðŸ“„ Support for multiple file formats (PDF, TXT, DOCX, MD)
- ðŸ”— URL content ingestion
- âš¡ Streaming responses
- ðŸ”’ Secure API key management
- ðŸ’¾ Vector database for efficient document retrieval

## Project Structure

```
rag-llm-chat/
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ rag_methods.py      # RAG functionality implementation
â”œâ”€â”€ requirements.txt    # Project dependencies
â”œâ”€â”€ .env               # Environment variables (not in repo)
â””â”€â”€ README.md          # This file
```

## Dependencies used

Key dependencies include:

- `streamlit`: Web application framework
- `langchain`: LLM framework and tools
- `chromadb`: Vector store for document embeddings
- `anthropic`: Claude model integration
- `openai`: OpenAI model integration
- `unstructured`: Document processing
- `python-magic`: File type detection
- `pdf2image`: PDF processing
- `pytesseract`: OCR capabilities
